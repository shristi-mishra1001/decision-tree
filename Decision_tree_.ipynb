{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Supervised Classification: Decision\n",
        "#Trees, SVM, and Naive Bayes|\n",
        "\n",
        "#**Assignment**"
      ],
      "metadata": {
        "id": "P7HkKAK3z9IN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 1 : What is Information Gain, and how is it used in Decision Trees?\n",
        "\n",
        "# Answer ->\n",
        "Information Gain is a measure used in decision tree algorithms to determine how well a feature splits the data into different classes. It calculates the reduction in entropy (uncertainty) after the dataset is split on a particular feature.\n",
        "\n",
        "In decision trees, the feature with the highest Information Gain is selected as the splitting node because it provides the most useful information for classification. This process is repeated at each node to build the complete decision tree.\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "ILjgQQcV0MAa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 2: What is the difference between Gini Impurity and Entropy?\n",
        "\n",
        "Hint: Directly compares the two main impurity measures, highlighting strengths,\n",
        "weaknesses, and appropriate use cases.\n",
        "\n",
        "**Answer **->\n",
        "\n",
        "  Gini Impurity and Entropy are both measures used in decision tree algorithms to evaluate how impure or mixed a dataset is. They help in selecting the best feature for splitting the data.\n",
        "\n",
        "Gini Impurity measures the probability that a randomly chosen sample would be incorrectly classified if it were randomly labeled according to the class distribution. It is computationally faster and is commonly used in the CART algorithm.\n",
        "\n",
        "Entropy measures the level of uncertainty or randomness in the dataset using a logarithmic calculation. It is more mathematically rigorous and is used in algorithms like ID3 and C4.5.\n",
        "\n",
        "In practice, Gini Impurity is preferred when efficiency and speed are important, while Entropy is used when a more informative and theoretically sound split is desired.\n",
        "\n",
        "| Aspect        | Gini Impurity                    | Entropy                            |\n",
        "| ------------- | -------------------------------- | ---------------------------------- |\n",
        "| Concept       | Probability of misclassification | Measure of uncertainty             |\n",
        "| Formula       | (1 - \\sum p_i^2)                 | (- \\sum p_i \\log_2 p_i)            |\n",
        "| Speed         | Faster to compute                | Slower due to log                  |\n",
        "| Used in       | CART                             | ID3, C4.5                          |\n",
        "| Best use case | Large datasets                   | When information gain is important |\n",
        "\n"
      ],
      "metadata": {
        "id": "l9uvVIDA1hki"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 3:What is Pre-Pruning in Decision Trees?\n",
        "\n",
        "Answer:->\n",
        "\n",
        " Pre-Pruning is a technique used in decision trees to stop the growth of the tree at an early stage in order to prevent overfitting. In this method, the decision tree is restricted from creating further splits if certain predefined conditions are met.\n",
        "\n",
        "Common stopping criteria in pre-pruning include setting a maximum tree depth, minimum number of samples required to split a node, or a minimum information gain threshold. By limiting the complexity of the tree, pre-pruning helps improve model generalization and reduces computational cost."
      ],
      "metadata": {
        "id": "PIvfNTi_2Ey6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 4:Write a Python program to train a Decision Tree Classifier using Gini\n",
        "#Impurity as the criterion and print the feature importances (practical).\n",
        "#Hint: Use criterion='gini' in DecisionTreeClassifier and access .feature_importances_.\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "Answer  ->\n"
      ],
      "metadata": {
        "id": "Ctv_yaUj4ruK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train Decision Tree Classifier using Gini Impurity\n",
        "dt = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "\n",
        "# Print feature importances\n",
        "print(\"Feature Importances:\")\n",
        "for feature, importance in zip(data.feature_names, dt.feature_importances_):\n",
        "    print(f\"{feature}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7yYQkFY5MPP",
        "outputId": "8c0762e6-d989-4a54-b755-6fb86b1dfeda"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances:\n",
            "sepal length (cm): 0.0000\n",
            "sepal width (cm): 0.0167\n",
            "petal length (cm): 0.9061\n",
            "petal width (cm): 0.0772\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 5: What is a Support Vector Machine (SVM)?\n",
        "\n",
        "Answer->  \n",
        "\n",
        " A Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression tasks. It works by finding an optimal hyperplane that best separates data points of different classes in a high-dimensional space.\n",
        "\n",
        "\n",
        "\n",
        "SVM focuses on the data points closest to the decision boundary, called support vectors, and maximizes the margin between classes. It can handle both linear and non-linear data using kernel functions such as linear, polynomial, and radial basis function (RBF).\n"
      ],
      "metadata": {
        "id": "y0zVwcqe5Ycy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 6: What is the Kernel Trick in SVM?\n",
        "\n",
        "Answer ->\n",
        "\n",
        " The Kernel Trick is a technique used in Support Vector Machines (SVM) to handle non-linearly separable data. It allows SVM to transform the original input data into a higher-dimensional feature space where a linear separation becomes possible, without explicitly computing the transformation.\n",
        "\n",
        "By using kernel functions such as linear, polynomial, and radial basis function (RBF), SVM efficiently finds the optimal separating hyperplane while reducing computational complexity."
      ],
      "metadata": {
        "id": "Tv53BA_n592I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 7: Write a Python program to train two SVM classifiers with Linear and RBF\n",
        "kernels on the Wine dataset, then compare their accuracies.\n",
        "Hint:Use SVC(kernel='linear') and SVC(kernel='rbf'), then compare accuracy scores after fitting\n",
        "on the same dataset.\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "Answer ->  "
      ],
      "metadata": {
        "id": "O3izXmki6ago"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Wine dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train SVM with Linear kernel\n",
        "svm_linear = SVC(kernel='linear')\n",
        "svm_linear.fit(X_train, y_train)\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "\n",
        "# Train SVM with RBF kernel\n",
        "svm_rbf = SVC(kernel='rbf')\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "\n",
        "# Calculate accuracies\n",
        "linear_accuracy = accuracy_score(y_test, y_pred_linear)\n",
        "rbf_accuracy = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "# Print results\n",
        "print(\"Linear Kernel Accuracy:\", linear_accuracy)\n",
        "print(\"RBF Kernel Accuracy:\", rbf_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_gtfW_w6vPY",
        "outputId": "b5e9be65-04c1-43da-e5b7-cab28a0fb5cb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Kernel Accuracy: 1.0\n",
            "RBF Kernel Accuracy: 0.8055555555555556\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 8: What is the Naïve Bayes classifier, and why is it called \"Naïve\"?\n",
        "\n",
        " Answer ->\n",
        "\n",
        " The Naïve Bayes classifier is a supervised machine learning algorithm based on Bayes’ Theorem and is mainly used for classification tasks. It calculates the probability of a data point belonging to a particular class based on the probabilities of its features.\n",
        "\n",
        "\n",
        "It is called “Naïve” because it assumes that all features are independent of each other, which is a strong and often unrealistic assumption. Despite this simplification, Naïve Bayes performs well in many real-world applications such as text classification, spam detection, and sentiment analysis.\n"
      ],
      "metadata": {
        "id": "reR871Sp67FA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 9: Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve\n",
        "#Bayes, and Bernoulli Naïve Bayes\n",
        "\n",
        "\n",
        "Answer ->\n",
        "\n",
        " Gaussian, Multinomial, and Bernoulli Naïve Bayes are different variants of the Naïve Bayes classifier, each designed for specific types of data distributions.\n",
        "\n",
        "Gaussian Naïve Bayes assumes that the features follow a normal (Gaussian) distribution. It is mainly used for continuous numerical data such as height, weight, or sensor values.\n",
        "\n",
        "Multinomial Naïve Bayes is used for discrete count data. It is widely applied in text classification problems like spam detection, where features represent word frequencies or term counts.\n",
        "\n",
        "Bernoulli Naïve Bayes works with binary features, where the presence or absence of a feature is considered. It is useful when features are represented as yes/no or 0/1 values, such as whether a word appears in a document or not."
      ],
      "metadata": {
        "id": "u8vp_0oD7RSo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 10: Breast Cancer Dataset\n",
        "Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer\n",
        "dataset and evaluate accuracy.\n",
        "Hint:Use GaussianNB() from sklearn.naive_bayes and the Breast Cancer dataset from\n",
        "sklearn.datasets.\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n"
      ],
      "metadata": {
        "id": "kpUb6epU8K3B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train Gaussian Naive Bayes classifier\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print accuracy\n",
        "print(\"Accuracy of Gaussian Naive Bayes Classifier:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "950F_OLx8mzQ",
        "outputId": "efd658e1-2a43-463b-ec9a-49191cd3ff48"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Gaussian Naive Bayes Classifier: 0.9736842105263158\n"
          ]
        }
      ]
    }
  ]
}